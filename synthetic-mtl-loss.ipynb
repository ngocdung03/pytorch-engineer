{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from lifelines.utils import concordance_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>label</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>feature11</th>\n",
       "      <th>feature12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.440500</td>\n",
       "      <td>-0.035066</td>\n",
       "      <td>-0.025341</td>\n",
       "      <td>-0.029775</td>\n",
       "      <td>-0.55787</td>\n",
       "      <td>-0.15355</td>\n",
       "      <td>0.56819</td>\n",
       "      <td>-0.15432</td>\n",
       "      <td>-0.250230</td>\n",
       "      <td>0.33915</td>\n",
       "      <td>0.70388</td>\n",
       "      <td>0.28174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>-0.846080</td>\n",
       "      <td>0.487530</td>\n",
       "      <td>0.651930</td>\n",
       "      <td>0.20099</td>\n",
       "      <td>-0.11238</td>\n",
       "      <td>-1.39630</td>\n",
       "      <td>-0.18874</td>\n",
       "      <td>-0.300010</td>\n",
       "      <td>-0.24032</td>\n",
       "      <td>-0.38533</td>\n",
       "      <td>-1.02450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0.446490</td>\n",
       "      <td>1.641000</td>\n",
       "      <td>-1.745000</td>\n",
       "      <td>0.317950</td>\n",
       "      <td>-1.14060</td>\n",
       "      <td>0.36560</td>\n",
       "      <td>0.28110</td>\n",
       "      <td>-0.58253</td>\n",
       "      <td>-1.690700</td>\n",
       "      <td>1.20220</td>\n",
       "      <td>-0.51920</td>\n",
       "      <td>1.78400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.629460</td>\n",
       "      <td>-0.615750</td>\n",
       "      <td>-0.323450</td>\n",
       "      <td>-0.900200</td>\n",
       "      <td>0.45360</td>\n",
       "      <td>-0.61992</td>\n",
       "      <td>2.16240</td>\n",
       "      <td>0.19875</td>\n",
       "      <td>-1.119600</td>\n",
       "      <td>-2.73210</td>\n",
       "      <td>-0.25673</td>\n",
       "      <td>-0.81836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.249800</td>\n",
       "      <td>-0.185610</td>\n",
       "      <td>-0.183780</td>\n",
       "      <td>-0.981080</td>\n",
       "      <td>-0.01499</td>\n",
       "      <td>-0.14437</td>\n",
       "      <td>-1.25290</td>\n",
       "      <td>-0.58432</td>\n",
       "      <td>-0.090523</td>\n",
       "      <td>0.93692</td>\n",
       "      <td>1.07490</td>\n",
       "      <td>0.79117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  label  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0     0      0 -0.440500 -0.035066 -0.025341 -0.029775  -0.55787  -0.15355   \n",
       "1     1      0  0.015579 -0.846080  0.487530  0.651930   0.20099  -0.11238   \n",
       "2    34      2  0.446490  1.641000 -1.745000  0.317950  -1.14060   0.36560   \n",
       "3     9      0  0.629460 -0.615750 -0.323450 -0.900200   0.45360  -0.61992   \n",
       "4     2      0  1.249800 -0.185610 -0.183780 -0.981080  -0.01499  -0.14437   \n",
       "\n",
       "   feature7  feature8  feature9  feature10  feature11  feature12  \n",
       "0   0.56819  -0.15432 -0.250230    0.33915    0.70388    0.28174  \n",
       "1  -1.39630  -0.18874 -0.300010   -0.24032   -0.38533   -1.02450  \n",
       "2   0.28110  -0.58253 -1.690700    1.20220   -0.51920    1.78400  \n",
       "3   2.16240   0.19875 -1.119600   -2.73210   -0.25673   -0.81836  \n",
       "4  -1.25290  -0.58432 -0.090523    0.93692    1.07490    0.79117  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/chl8856/DeepHit/master/sample%20data/SYNTHETIC/synthetic_comprisk.csv'\n",
    "dataset = pd.read_csv(url)\n",
    "dataset.drop(['true_time', 'true_label'], axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [26.,  0.,  0.],\n",
       "       [33.,  0.,  1.],\n",
       "       ...,\n",
       "       [ 8.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [44.,  0.,  1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_x = lambda df: (df\n",
    "                    .drop(columns=['time', 'label'])\n",
    "                    .values.astype('float32'))\n",
    "\n",
    "df_test = dataset.sample(frac=0.2)\n",
    "df_train = dataset.drop(df_test.index)\n",
    "# df_val = df_train.sample(frac=0.2)\n",
    "# df_train = df_train.drop(df_val.index)\n",
    "\n",
    "X_train = get_x(df_train)\n",
    "X_test = get_x(df_test)\n",
    "\n",
    "Y_train = df_train[['label', 'time']].to_numpy()\n",
    "Y_test = df_test[['label', 'time']].to_numpy()\n",
    "\n",
    "# One-hot encoding \n",
    "# Train\n",
    "label_train = np.zeros((len(df_train['label']), df_train['label'].max()+1))\n",
    "label_train[np.arange(len(df_train['label'])), df_train['label']] = 1\n",
    "label_train = np.column_stack((np.array(df_train['time']),\n",
    "                             label_train[:,1:]))\n",
    "label_train\n",
    "\n",
    "# Test\n",
    "label_test = np.zeros((len(df_test['label']), df_test['label'].max()+1))\n",
    "label_test[np.arange(len(df_test['label'])), df_test['label']] = 1\n",
    "label_test = np.column_stack((np.array(df_test['time']),\n",
    "                             label_test[:,1:]))\n",
    "label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = X_train.shape[1]   \n",
    "batch_size = 32\n",
    "num_time_units = 10 \n",
    "n_epochs = 3\n",
    "learning_rate = 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Network\n",
    "#### DeepHit (rewritten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class deephit(nn.Module):\n",
    "    \"\"\" Deep network for competing risks in survival analysis (based on DeepHit)\n",
    "\n",
    "    Args:\n",
    "        in_features (int): number of covariates\n",
    "        hidden_layers (list): size of each hidden layer = list element * in_features\n",
    "        out_features (list): number of outputs for each event\n",
    "        p_dropout (float): probability of dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features = n_feature, hidden_layers = [30, 30, 50], out_features=[1, 1], p_dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_layers[0]* n_feature),  \n",
    "            nn.BatchNorm1d(hidden_layers[0]* n_feature),\n",
    "            nn.ReLU(), \n",
    "            # nn.Dropout(p_dropout) \n",
    "        ) \n",
    " \n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[0]* n_feature + in_features, hidden_layers[1]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[1]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p_dropout),\n",
    "        \n",
    "            nn.Linear(hidden_layers[1]*n_feature, hidden_layers[2]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[2]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            \n",
    "            nn.ReLU(),  #\n",
    "            \n",
    "            nn.Linear(hidden_layers[2]*n_feature, out_features[0]),\n",
    "        )\n",
    "        self.fc_layer1 = nn.Linear(out_features[0], num_time_units)\n",
    "        \n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[0]* n_feature + in_features, hidden_layers[1]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[1]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p_dropout),\n",
    "        \n",
    "            nn.Linear(hidden_layers[1]*n_feature, hidden_layers[2]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[2]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            \n",
    "            nn.ReLU(),  #\n",
    "            \n",
    "            nn.Linear(hidden_layers[2]*n_feature, out_features[1]),\n",
    "        )\n",
    "        self.fc_layer2 = nn.Linear(out_features[1], num_time_units)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = nn.init.xavier_uniform(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        shared = self.sharedlayer(x)\n",
    "        \n",
    "        # Residual concatenating\n",
    "        shared = torch.concat((shared, residual), dim=1) \n",
    "       \n",
    "        out1 = self.task1(shared)\n",
    "        score1_1 = out1   # torch.exp(x.mm(out))\n",
    "        # score1_2 = torch.sigmoid(self.fc_layer1(score1_1))   # For predicting survival\n",
    "        \n",
    "        out2 = self.task2(shared)\n",
    "        score2_1 = out2\n",
    "        # score2_2 = torch.sigmoid(self.fc_layer2(score2_1)) \n",
    "        return [score1_1, score2_1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with one more share block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plusshare(nn.Module):\n",
    "    \"\"\" One more block for share network\n",
    "\n",
    "    Args:\n",
    "        in_features (int): number of covariates\n",
    "        hidden_layers (list): size of each hidden layer = list element * in_features\n",
    "        out_features (list): number of outputs for each event\n",
    "        p_dropout (float): probability of dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features = n_feature, hidden_layers = [30, 30, 50], out_features=[1, 1], p_dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_layers[0]* n_feature),  \n",
    "            nn.BatchNorm1d(hidden_layers[0]* n_feature),\n",
    "            nn.ReLU(), \n",
    "            # nn.Dropout(p_dropout) \n",
    "            \n",
    "            nn.Linear(hidden_layers[0]* n_feature, hidden_layers[0]* n_feature),  \n",
    "            nn.BatchNorm1d(hidden_layers[0]* n_feature),\n",
    "            nn.ReLU(), \n",
    "        ) \n",
    " \n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[0]* n_feature + in_features, hidden_layers[1]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[1]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p_dropout),\n",
    "        \n",
    "            nn.Linear(hidden_layers[1]*n_feature, hidden_layers[2]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[2]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            \n",
    "            nn.ReLU(),  #\n",
    "            \n",
    "            nn.Linear(hidden_layers[2]*n_feature, out_features[0]),\n",
    "        )\n",
    "        self.fc_layer1 = nn.Linear(out_features[0], num_time_units)\n",
    "        \n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[0]* n_feature + in_features, hidden_layers[1]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[1]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p_dropout),\n",
    "        \n",
    "            nn.Linear(hidden_layers[1]*n_feature, hidden_layers[2]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[2]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            \n",
    "            nn.ReLU(),  #\n",
    "            \n",
    "            nn.Linear(hidden_layers[2]*n_feature, out_features[1]),\n",
    "        )\n",
    "        self.fc_layer2 = nn.Linear(out_features[1], num_time_units)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = nn.init.xavier_uniform(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        shared = self.sharedlayer(x)\n",
    "        \n",
    "        # Residual concatenating\n",
    "        shared = torch.concat((shared, residual), dim=1) \n",
    "       \n",
    "        out1 = self.task1(shared)\n",
    "        score1_1 = out1   # torch.exp(x.mm(out))\n",
    "        # score1_2 = torch.sigmoid(self.fc_layer1(score1_1))   # For predicting survival\n",
    "        \n",
    "        out2 = self.task2(shared)\n",
    "        score2_1 = out2\n",
    "        # score2_2 = torch.sigmoid(self.fc_layer2(score2_1)) \n",
    "        return [score1_1, score2_1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change FC into Conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6 ,5)  #(input color channel of 3, output channel of 6, kernel size)\n",
    "        self.pool = nn.MaxPool2d(2, 2) #(kernel size=2, stride=2) -> reduce the image by a factor of 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) #(= output channel size of last layer,_,_)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120) #(flattened size of the output tensor of last layer,_)\n",
    "        self.fc2 = nn.Linear(120, 84) \n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32  #batch size, color channel, image size, image size\n",
    "        x = self.pool(F.relu(self.conv1(x))) # -> n, 6, 14, 14  # 1st activation\n",
    "        x = self.pool(F.relu(self.conv2(x))) # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16*5*5)              # -> n, 400  # flatten the tensor\n",
    "        x = F.relu(self.fc1(x))             # -> n, 120  # fully connected layer from now\n",
    "        x = F.relu(self.fc2(x))             # -> n, 84\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convo(nn.Module):\n",
    "    \"\"\" Change fully connected layers into convolutional layers\n",
    "\n",
    "    Args:\n",
    "        in_features (int): number of covariates\n",
    "        hidden_layers (list): size of each hidden layer = list element * in_features\n",
    "        out_features (list): number of outputs for each event\n",
    "        p_dropout (float): probability of dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=n_feature, hidden_layers = [30, 30, 50], out_features=[1, 1], p_dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=n_feature, out_channels=hidden_layers[0]* n_feature, kernel_size=3),  \n",
    "            nn.BatchNorm1d(hidden_layers[0]* n_feature),\n",
    "            nn.ReLU(), \n",
    "            # nn.Dropout(p_dropout) \n",
    "        ) \n",
    " \n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[0]* n_feature + in_features, hidden_layers[1]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[1]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p_dropout),\n",
    "        \n",
    "            nn.Linear(hidden_layers[1]*n_feature, hidden_layers[2]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[2]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            \n",
    "            nn.ReLU(),  #\n",
    "            \n",
    "            nn.Linear(hidden_layers[2]*n_feature, out_features[0]),\n",
    "        )\n",
    "        self.fc_layer1 = nn.Linear(out_features[0], num_time_units)\n",
    "        \n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[0]* n_feature + in_features, hidden_layers[1]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[1]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p_dropout),\n",
    "        \n",
    "            nn.Linear(hidden_layers[1]*n_feature, hidden_layers[2]*n_feature),\n",
    "            nn.BatchNorm1d(hidden_layers[2]*n_feature),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            \n",
    "            nn.ReLU(),  #\n",
    "            \n",
    "            nn.Linear(hidden_layers[2]*n_feature, out_features[1]),\n",
    "        )\n",
    "        self.fc_layer2 = nn.Linear(out_features[1], num_time_units)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = nn.init.xavier_uniform(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        shared = self.sharedlayer(x)\n",
    "        \n",
    "        # Residual concatenating\n",
    "        shared = torch.concat((shared, residual), dim=1) \n",
    "       \n",
    "        out1 = self.task1(shared)\n",
    "        score1_1 = out1   # torch.exp(x.mm(out))\n",
    "        # score1_2 = torch.sigmoid(self.fc_layer1(score1_1))   # For predicting survival\n",
    "        \n",
    "        out2 = self.task2(shared)\n",
    "        score2_1 = out2\n",
    "        # score2_2 = torch.sigmoid(self.fc_layer2(score2_1)) \n",
    "        return [score1_1, score2_1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onePair(x0, x1):\n",
    "    c = np.log(2.)\n",
    "    m = nn.LogSigmoid() \n",
    "    return 1 + m(x1-x0) / c\n",
    "  \n",
    "def rank_loss(pred, obs, delta):\n",
    "    N = pred.size(0)\n",
    "    allPairs = onePair(pred.view(N,1), pred.view(1,N))\n",
    "\n",
    "    temp0 = obs.view(1, N) - obs.view(N, 1)\n",
    "    # indices based on obs time\n",
    "    temp1 = temp0>0\n",
    "    # indices of event-event or event-censor pair\n",
    "    temp2 = delta.view(1, N) + delta.view(N, 1)\n",
    "    temp3 = temp2>0\n",
    "    # indices of events\n",
    "    temp4 = delta.view(N, 1) * torch.ones(1, N, device = device)\n",
    "    # selected indices\n",
    "    final_ind = temp1 * temp3 * temp4\n",
    "    out = allPairs * final_ind\n",
    "    return out.sum() / final_ind.sum()\n",
    "\n",
    "def mse_loss(pred,  obs, delta):\n",
    "    mse = delta*((pred - obs) ** 2)\n",
    "\n",
    "    ind = pred < obs\n",
    "    delta0 = 1 - delta\n",
    "    p = ind * delta0 * (obs - pred)**2 \n",
    "    return mse.mean(), p.mean()\n",
    "\n",
    "def loss_func(pred, lifetime, event, lambda1 = 1, lambda2 = 0.2):\n",
    "    mseloss, penaltyloss = mse_loss(pred, lifetime.unsqueeze(1), event.unsqueeze(1))\n",
    "    rankloss = rank_loss(pred, lifetime.unsqueeze(1), event.unsqueeze(1))\n",
    "    loss = mseloss + lambda1*penaltyloss - lambda2*rankloss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation(model_instance):\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train).float().to(device), \n",
    "                                torch.from_numpy(label_train[:,0]).float().to(device), # time\n",
    "                                torch.from_numpy(label_train[:,1]).float().to(device), # event 1\n",
    "                                torch.from_numpy(label_train[:,2]).float().to(device)) # event 2\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X_test).float().to(device), \n",
    "                                torch.from_numpy(label_test[:,0]).float().to(device), # time\n",
    "                                torch.from_numpy(label_test[:,1]).float().to(device), # event 1\n",
    "                                torch.from_numpy(label_test[:,2]).float().to(device)) # event 2\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    model = model_instance\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-8)\n",
    "\n",
    "    # Training\n",
    "    # epoch_loss_train = []\n",
    "    for e in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        for X_train_batch, lifetime_batch, event1_batch, event2_batch, in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            yhat1, yhat2 = model(X_train_batch)\n",
    "            loss1 = loss_func(pred = yhat1, lifetime=lifetime_batch, event=event1_batch)\n",
    "            loss2 = loss_func(pred = yhat2, lifetime=lifetime_batch, event=event2_batch)\n",
    "            train_loss = loss1 + loss2\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Predicting train\n",
    "    train_loader1 = DataLoader(dataset=train_dataset, batch_size=len(train_dataset))\n",
    "    y_pred_list0_1 = []\n",
    "    y_pred_list0_2 = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X_batch, lifetime_batch, event1_batch, event2_batch in train_loader1:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred1, y_test_pred2 = model(X_batch)\n",
    "            y_pred_list0_1.append(y_test_pred1.cpu().numpy())\n",
    "            y_pred_list0_2.append(y_test_pred2.cpu().numpy())\n",
    "    y_pred_list0_1 = [a.squeeze().tolist() for a in y_pred_list0_1]\n",
    "    y_pred_list0_1 = sum(y_pred_list0_1, [])\n",
    "    y_pred_list0_2 = [a.squeeze().tolist() for a in y_pred_list0_2]\n",
    "    y_pred_list0_2 = sum(y_pred_list0_2, [])\n",
    "\n",
    "    # Predicting test\n",
    "    with torch.no_grad():\n",
    "        model.train() \n",
    "        result1 = []\n",
    "        result2 = []\n",
    "        for _ in range(100):   \n",
    "            y_pred_list_1 = []\n",
    "            y_pred_list_2 = [] \n",
    "            for X_batch, lifetime_batch, even1_batch, event2_batch in test_loader:\n",
    "                y_test_pred1, y_test_pred2 = model(X_batch)\n",
    "                y_pred_list_1.append(y_test_pred1.cpu().numpy())\n",
    "                y_pred_list_1 = [a.squeeze().tolist() for a in y_pred_list_1]\n",
    "                y_pred_list_1 = sum(y_pred_list_1, [])\n",
    "                \n",
    "                y_pred_list_2.append(y_test_pred2.cpu().numpy())\n",
    "                y_pred_list_2 = [a.squeeze().tolist() for a in y_pred_list_2]\n",
    "                y_pred_list_2 = sum(y_pred_list_2, [])\n",
    "            result1.append(y_pred_list_1)\n",
    "            result2.append(y_pred_list_2)\n",
    "\n",
    "        # result = np.array(result)\n",
    "        # y_test_pred_mean = result.mean(axis=0).reshape(-1,)\n",
    "        # y_test_pred_sd = result.std(axis=0).reshape(-1,)\n",
    "        # y_pred_list_upper = y_test_pred_mean + 1.96*y_test_pred_sd\n",
    "        # y_pred_list_lower = y_test_pred_mean - 1.96*y_test_pred_sd\n",
    "        \n",
    "\n",
    "    print(\"Train C-index for event 1: \", concordance_index(label_train[:,0], \n",
    "                                                            np.exp(y_pred_list0_1),\n",
    "                                                            label_train[:,1]))\n",
    "\n",
    "    print(\"Train C-index for event 2: \", concordance_index(label_train[:,0], \n",
    "                                                            np.exp(y_pred_list0_2),\n",
    "                                                            label_train[:,2]))\n",
    "\n",
    "    print(\"Test C-index for event 1: \", concordance_index(label_test[:,0], \n",
    "                                                            np.exp(y_pred_list_1),\n",
    "                                                            Y_test[:,0]))\n",
    "\n",
    "    print(\"Test C-index for event 2: \", concordance_index(Y_test[:,1], \n",
    "                                                            np.exp(y_pred_list_2),\n",
    "                                                            Y_test[:,0]))\n",
    "    # print(\"\")\n",
    "    # print(\"Train MSE: \", mean_squared_error(np.log(train_df[\"FT\"]), y_train_pred))\n",
    "    # print(\"Test MSE: \", mean_squared_error(np.log(test_df[\"FT\"]), y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepHit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nnd/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train C-index for event 1:  0.7328074905969261\n",
      "Train C-index for event 2:  0.7360915498663\n",
      "Test C-index for event 1:  0.7215800631836892\n",
      "Test C-index for event 2:  0.7203477989394438\n"
     ]
    }
   ],
   "source": [
    "evaluation(deephit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One more block for share network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nnd/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train C-index for event 1:  0.7285313151385455\n",
      "Train C-index for event 2:  0.7358306698973992\n",
      "Test C-index for event 1:  0.7206152421218518\n",
      "Test C-index for event 2:  0.7210531526392243\n"
     ]
    }
   ],
   "source": [
    "evaluation(plusshare())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change FC into Conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nnd/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight [360, 12, 3], but got 2-dimensional input of size [32, 12] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w7/g10s9mmx02935sgky924x5g40000gn/T/ipykernel_9962/1251967324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/w7/g10s9mmx02935sgky924x5g40000gn/T/ipykernel_9962/3685339056.py\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0myhat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myhat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlifetime_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent1_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myhat2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlifetime_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/w7/g10s9mmx02935sgky924x5g40000gn/T/ipykernel_9962/3299977539.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mshared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharedlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Residual concatenating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    297\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 298\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [360, 12, 3], but got 2-dimensional input of size [32, 12] instead"
     ]
    }
   ],
   "source": [
    "evaluation(convo())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w7/g10s9mmx02935sgky924x5g40000gn/T/ipykernel_9962/717157382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'in_features'"
     ]
    }
   ],
   "source": [
    "sample = convo()\n",
    "sample(torch.tensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e2b3cd2829f09aa4cbce26e91031c8f0c76a211d73914c4bed7f6ce2202e3c6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

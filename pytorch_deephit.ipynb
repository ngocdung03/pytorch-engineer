{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/chl8856/DeepHit/master/sample%20data/SYNTHETIC/synthetic_comprisk.csv'\n",
    "df_train = pd.read_csv(url)\n",
    "df_train.drop(['true_time', 'true_label'], axis=1, inplace=True)\n",
    "df_train.head()\n",
    "\n",
    "df_test = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_test.index)\n",
    "# df_val = df_train.sample(frac=0.2)   #? 5 fold CV\n",
    "# df_train = df_train.drop(df_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_x = lambda df: (df\n",
    "                    .drop(columns=['time', 'label'])\n",
    "                    .values.astype('float32'))\n",
    "\n",
    "x_train = get_x(df_train)\n",
    "# x_val = get_x(df_val)\n",
    "x_test = get_x(df_test)\n",
    "\n",
    "# Label transform\n",
    "class LabTransform(LabTransDiscreteTime):\n",
    "    def transform(self, durations, events):\n",
    "        durations, is_event = super().transform(durations, events > 0)\n",
    "        events[is_event == 0] = 0\n",
    "        return durations, events.astype('int64')\n",
    "    \n",
    "num_durations = 10\n",
    "labtrans = LabTransform(num_durations)\n",
    "get_target = lambda df: (df['time'].values, df['label'].values)\n",
    "\n",
    "y_train = labtrans.fit_transform(*get_target(df_train))\n",
    "# y_val = labtrans.transform(*get_target(df_val))\n",
    "y_test = labtrans.transform(*get_target(df_test))\n",
    "durations_test, events_test = get_target(df_test)\n",
    "len(y_train[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding \n",
    "label_train = np.zeros((len(y_train[1]), y_train[1].max()+1))\n",
    "label_train[np.arange(len(y_train[1])), y_train[1]] = 1\n",
    "label_train\n",
    "\n",
    "# Test\n",
    "label_test = np.zeros((len(y_test[1]), y_test[1].max()+1))\n",
    "label_test[np.arange(len(y_test[1])), y_test[1]] = 1\n",
    "label_test\n",
    "\n",
    "#? Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "random_seed = 123\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "n_feature = 12\n",
    "num_time_units = 10   # 10 months\n",
    "time_bin = 30  # ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lis = [1, 3, 5, 6, 2, ]\n",
    " \n",
    "# # using reduce to compute sum of list\n",
    "# print(\"The sum of the list elements is : \", end=\"\")\n",
    "# print(reduce(lambda a, b: a+b, lis))\n",
    " \n",
    "# # using reduce to compute maximum element from list\n",
    "# print(\"The maximum element of the list is : \", end=\"\")\n",
    "# print(reduce(lambda a, b: a if a > b else b, lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def unique_set(lifetime):\n",
    "    a = lifetime.numpy()   # lifetime.data.cpu().numpy()\n",
    "    t, idx = np.unique(a, return_inverse=True)\n",
    "    sort_idx = np.argsort(a)\n",
    "    a_sorted = a[sort_idx]\n",
    "    unq_first = np.concatenate(([True], a_sorted[1:] != a_sorted[:-1]))\n",
    "    unq_count = np.diff(np.nonzero(unq_first)[0])\n",
    "    unq_idx = np.split(sort_idx, np.cumsum(unq_count))\n",
    "    return t, unq_idx\n",
    "    \n",
    "def log_parlik(lifetime, censor, score1):  \n",
    "    t, H = unique_set(lifetime)\n",
    "    keep_index = np.nonzero(censor.numpy())[0]  #censor = 1  #.data.cpu()\n",
    "    H = [list(set(h)&set(keep_index)) for h in H]\n",
    "    n = [len(h) for h in H]\n",
    "    \n",
    "    score1 = score1.detach().numpy()   # .data.cpu()  \n",
    "    total = 0\n",
    "    for j in range(len(t)):\n",
    "        total_1 = np.sum(np.log(score1)[H[j]])\n",
    "        m = n[j]\n",
    "        total_2 = 0\n",
    "        for i in range(m):\n",
    "            subtotal = np.sum(score1[sum(H[j:],[])]) - (i*1.0/m)*(np.sum(score1[H[j]]))\n",
    "            subtotal = np.log(subtotal)\n",
    "            total_2 = total_2 + subtotal\n",
    "        total = total + total_1 - total_2\n",
    "        total = np.array([total])\n",
    "    return torch.from_numpy(total).type(torch.FloatTensor).view(-1,1)\n",
    "        \n",
    "\n",
    "def acc_pairs(censor, lifetime):\n",
    "    noncensor_index = np.nonzero(censor.numpy())[0]  #.data.cpu()\n",
    "    lifetime = lifetime.numpy()  # .data.cpu()\n",
    "    acc_pair = []\n",
    "    for i in noncensor_index:\n",
    "        all_j =  np.array(range(len(lifetime)))[lifetime > lifetime[i]]\n",
    "        acc_pair.append([(i,j) for j in all_j])\n",
    "    \n",
    "    acc_pair = reduce(lambda x,y: x + y, acc_pair)\n",
    "    return acc_pair\n",
    "\n",
    "\n",
    "def rank_loss(lifetime, censor, score2, t, time_bin): \n",
    "    # score2 (n(samples)*24) at time unit t = 1,2,...,24\n",
    "    acc_pair = acc_pairs(censor, lifetime)\n",
    "    lifetime = lifetime.numpy()   #.data.cpu()\n",
    "    total = 0\n",
    "    for i,j in acc_pair:\n",
    "        yi = (lifetime[i] >= (t-1) * time_bin) * 1\n",
    "        yj = (lifetime[j] >= (t-1) * time_bin) * 1\n",
    "        a = torch.ones(1).type(torch.FloatTensor)\n",
    "        L2dist = torch.dist(score2[j, t-1] - score2[i, t-1], a, 2)\n",
    "        total = total + L2dist* yi * (1-yj)\n",
    "    return total\n",
    "\n",
    "\n",
    "def C_index(censor, lifetime, score1):\n",
    "    score1 = score1.detach().numpy()  #.data.cpu()  #?\n",
    "    acc_pair = acc_pairs(censor, lifetime)\n",
    "    prob = sum([score1[i] >= score1[j] for (i, j) in acc_pair])[0]*1.0/len(acc_pair)\n",
    "    return prob\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss criterion for multi-task learning\n",
    "\n",
    "# def criterion(score1, score2, lifetime, censor):\n",
    "#     for task in range(len(score1)):\n",
    "#         loss1 = log_parlik(lifetime, censor[task], score1[task])\n",
    "#         loss2 = []\n",
    "#         for t in range(num_time_units):\n",
    "#             loss2.append(rank_loss(lifetime, censor[task], score2[task], t+1, time_bin))\n",
    "#         loss2 = sum(loss2)\n",
    "#         loss = 1.0 * loss1 + 0.5 * loss2\n",
    "#     return torch.mean(loss)\n",
    "\n",
    "def criterion(score1, score2, lifetime, censor):\n",
    "    loss1 = log_parlik(lifetime, censor, score1)\n",
    "    loss2 = []\n",
    "    for t in range(num_time_units):\n",
    "        loss2.append(rank_loss(lifetime, censor, score2, t+1, time_bin))\n",
    "    loss2 = sum(loss2)\n",
    "    loss = 1.0 * loss1 + 0.5 * loss2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network defining\n",
    "\n",
    "* DeepHit is a 4-layer network consisting of 1 fully-connected layer for the shared sub-network and 2 fully- connected layers for each cause-specific sub-network and a softmax layer as the output layer.\n",
    "\n",
    "* For hidden layers, the number of nodes were set as 3, 5, and 3 times of the covariate dimension for the layer 1, 2, and 3, respectively, with ReLu activation function. \n",
    "\n",
    "* The network was trained by back-propagation via Adam optimizer with a batch size of 50 and a learning rate of 10âˆ’4. \n",
    "\n",
    "* Dropout probability of 0.6 and Xavier initialization was applied for all the layers (DeepHit was implemented in a Tensorflow environment).\n",
    "\n",
    "* Ref of multitask learning in Pytorch:\n",
    "    * https://github.com/Hui-Li/multi-task-learning-example-PyTorch/blob/master/multi-task-learning-example-PyTorch.ipynb\n",
    "    * https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example-pytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network    \n",
    "class SpecificNet(nn.Module):\n",
    "    def __init__(self, in_features = n_feature, hidden_layers = [3, 3, 5], out_features=[1, 1], alpha=0.2, sigma=0.1, p_dropout=0.6, loss=None):\n",
    "        super().__init__()\n",
    "        self.sharedlayer = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_layers[0]* n_feature),  # 3*n_feature\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_dropout) \n",
    "            # one output layer?\n",
    "        ) \n",
    " \n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[0]* n_feature, hidden_layers[1]*n_feature),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_dropout),\n",
    "        \n",
    "            nn.Linear(hidden_layers[1]*n_feature, hidden_layers[2]*n_feature),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_dropout),\n",
    "            \n",
    "            nn.Linear(hidden_layers[2]*n_feature, out_features[0]),\n",
    "        )\n",
    "        self.fc_layer1 = nn.Linear(out_features[0], num_time_units)\n",
    "        \n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[0]* n_feature, hidden_layers[1]*n_feature),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_dropout),\n",
    "        \n",
    "            nn.Linear(hidden_layers[1]*n_feature, hidden_layers[2]*n_feature),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_dropout),\n",
    "            \n",
    "            nn.Linear(hidden_layers[2]*n_feature, out_features[1]),\n",
    "        )\n",
    "        self.fc_layer2 = nn.Linear(out_features[1], num_time_units)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = nn.init.xavier_uniform(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        shared = self.sharedlayer(x)\n",
    "        # shared = torch.concat(shared, residual, dim = 1) # size mapping?     \n",
    "        # torch.concat(out1, out2, dim = 1) \n",
    "        out1 = self.task1(shared)\n",
    "        score1_1 = torch.exp(out1)   # torch.exp(x.mm(out))\n",
    "        score1_2 = torch.sigmoid(self.fc_layer1(score1_1)) \n",
    "        \n",
    "        out2 = self.task2(shared)\n",
    "        score2_1 = torch.exp(out2)\n",
    "        score2_2 = torch.sigmoid(self.fc_layer2(score2_1)) \n",
    "        return [score1_1, score1_2], [score2_1, score2_2]  # dim = 0 for softmax # torch.concat(out1, out2, dim = 1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[8.8227e-01],\n",
      "        [1.0642e-04],\n",
      "        [6.4972e+00],\n",
      "        ...,\n",
      "        [4.2415e-01],\n",
      "        [1.7161e+01],\n",
      "        [2.9226e+08]], grad_fn=<ExpBackward0>), tensor([[0.3583, 0.4382, 0.3802,  ..., 0.3225, 0.6274, 0.5640],\n",
      "        [0.3528, 0.4913, 0.3412,  ..., 0.3584, 0.4078, 0.4588],\n",
      "        [0.3946, 0.1669, 0.6430,  ..., 0.1466, 0.9980, 0.9502],\n",
      "        ...,\n",
      "        [0.3555, 0.4656, 0.3597,  ..., 0.3409, 0.5142, 0.5095],\n",
      "        [0.4664, 0.0149, 0.9330,  ..., 0.0242, 1.0000, 0.9997],\n",
      "        [1.0000, 0.0000, 1.0000,  ..., 0.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SigmoidBackward0>)]\n",
      "torch.Size([24000, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nnd/opt/anaconda3/envs/deephit-env/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "# Weight initialization\n",
    "# SpecificNet.apply(init_weights)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model1 = SpecificNet()\n",
    "a, b = model1(torch.Tensor(x_train))\n",
    "print(a)\n",
    "print(b[1].shape)\n",
    "# score1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w7/g10s9mmx02935sgky924x5g40000gn/T/ipykernel_92836/395658013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 3, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tensor(y_train[0]).view(-1)\n",
    "y_train[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 34,  9, ...,  2, 68,  4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2 = df_train.reset_index().to_numpy()\n",
    "idx = np.random.permutation(x_train.shape[0])\n",
    "df_train['time'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating\n",
    "- Ref:  https://github.com/jysonganan/DeepLearningSurvival/blob/master/DeepSurv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = TrainData(feature_num, X, Y1, Y2)\n",
    "# train_data_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 training loss: -3.4842\n",
      "====> Epoch: 1 testing loss: -1.0980\n",
      "====> Epoch: 2 training loss: -3.5079\n",
      "====> Epoch: 2 testing loss: -1.0980\n",
      "====> Epoch: 3 training loss: -3.4521\n",
      "====> Epoch: 3 testing loss: -1.0980\n",
      "====> Epoch: 4 training loss: -3.4347\n",
      "====> Epoch: 4 testing loss: -1.0980\n",
      "====> Epoch: 5 training loss: -3.4183\n",
      "====> Epoch: 5 testing loss: -1.0980\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import time\n",
    "start_time = time.time()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "time_train = df_train['time'].to_numpy()\n",
    "time_test = df_test['time'].to_numpy()\n",
    "def train(epoch):\n",
    "    \n",
    "    model1.train()\n",
    "    train_loss = 0\n",
    "    idx = np.random.permutation(x_train.shape[0])\n",
    "    j = 0\n",
    "    while j < x_train.shape[0]:\n",
    "        if j < x_train.shape[0] - batch_size:\n",
    "            data = torch.from_numpy(x_train[idx[j:(j + batch_size)]]).type(torch.FloatTensor)\n",
    "            lifetime = torch.from_numpy(time_train[idx[j:(j + batch_size)]]).type(torch.FloatTensor)\n",
    "            censor = torch.from_numpy(label_train[idx[j:(j + batch_size)],1:]).type(torch.FloatTensor)\n",
    "        else:\n",
    "            data = torch.from_numpy(x_train[idx[j:]]).type(torch.FloatTensor)\n",
    "            lifetime = torch.from_numpy(time_train[idx[j:]]).type(torch.FloatTensor)\n",
    "            censor = torch.from_numpy(label_train[idx[j:],1:]).type(torch.FloatTensor)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        score_task1, score_task2 = model1(data)\n",
    "        loss_1 = criterion(score_task1[0], score_task1[1], lifetime, censor)\n",
    "        loss_2 = criterion(score_task2[0], score_task2[1], lifetime, censor)\n",
    "        loss = (loss_1 + loss_2)/2\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()      \n",
    "        # train_loss = loss.data[0]\n",
    "        optimizer.step()\n",
    "        j += batch_size\n",
    "    return train_loss*1.0 / x_train.shape[0]\n",
    "\n",
    "durations_test, events_test \n",
    "\n",
    "def test(epoch):\n",
    "    \n",
    "    model1.eval()\n",
    "    test_loss = 0\n",
    "    j = 0\n",
    "    while j < x_test.shape[0]:\n",
    "        if j < x_test.shape[0] - batch_size:\n",
    "            data = torch.from_numpy(x_test[j:(j + batch_size)]).type(torch.FloatTensor)\n",
    "            lifetime = torch.from_numpy(time_test[j:(j + batch_size)]).type(torch.FloatTensor)\n",
    "            censor = torch.from_numpy(label_test[j:(j + batch_size),1:]).type(torch.FloatTensor)\n",
    "        else:\n",
    "            data = torch.from_numpy(x_test[j:]).type(torch.FloatTensor)\n",
    "            lifetime = torch.from_numpy(time_test[j:]).type(torch.FloatTensor)\n",
    "            censor = torch.from_numpy(label_test[j:,1:]).type(torch.FloatTensor)\n",
    "            \n",
    "        score_task1, score_task2 = model1(data)\n",
    "        loss_1 = criterion(score_task1[0], score_task1[1], lifetime, censor)\n",
    "        loss_2 = criterion(score_task2[0], score_task2[1], lifetime, censor)\n",
    "        loss = (loss_1 + loss_2)/2\n",
    "        \n",
    "        # loss = criterion(score1, score2, lifetime, censor)\n",
    "        test_loss += loss.item()\n",
    "        # test_loss += loss.data[0]\n",
    "        j += batch_size\n",
    "        \n",
    "    return test_loss*1.0 / x_test.shape[0]\n",
    "        \n",
    "    \n",
    "    \n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    print('====> Epoch: %d training loss: %.4f'%(epoch, train_loss))\n",
    "    print('====> Epoch: %d testing loss: %.4f'%(epoch, test_loss))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance index for training data: 0.4910\n",
      "Concordance index for test data: 0.4859\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "# optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "# time_train = df_train['time'].to_numpy()\n",
    "# time_test = df_test['time'].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# concordance - training\n",
    "data_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "lifetime_train = torch.from_numpy(time_train).type(torch.FloatTensor)\n",
    "censor_train = torch.from_numpy(label_train[:,1]).type(torch.FloatTensor)\n",
    "\n",
    "score_task1_train, score_task2_train = model1(data_train)\n",
    "C_index_train = C_index(censor_train, lifetime_train, score_task1_train[0])\n",
    "print('Concordance index for training data: {:.4f}'.format(C_index_train))\n",
    "\n",
    "\n",
    "# concordance - test\n",
    "data_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "lifetime_test = torch.from_numpy(time_test).type(torch.FloatTensor)\n",
    "censor_test = torch.from_numpy(label_test[:,1]).type(torch.FloatTensor)\n",
    "\n",
    "score_task1_test, score_task2_test = model1(data_test)\n",
    "C_index_test = C_index(censor_test, lifetime_test, score_task1_test[0])\n",
    "print('Concordance index for test data: {:.4f}'.format(C_index_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1f9b6552a3421f5225bac35aed37314e922b8de486337dc15548b014edef128"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
